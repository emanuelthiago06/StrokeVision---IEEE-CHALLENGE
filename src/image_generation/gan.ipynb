{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.api as keras\n",
    "from keras.api import layers\n",
    "from keras.api.layers import Conv2D, LeakyReLU, Flatten, Dropout, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (128, 128)\n",
    "channel = 1\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 64\n",
    "latent_dim_size = 1024\n",
    "dataset_path = \"../../data/train/\"\n",
    "complete_hist = {\n",
    "    'loss_dis': [],\n",
    "    'loss_gen': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "def loss_discriminador(real_output, fake_output):\n",
    "    real_loss = keras.losses.BinaryCrossentropy()(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def loss_gerador(fake_output):\n",
    "    return keras.losses.BinaryCrossentropy()(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def create_generator():\n",
    "\n",
    "    input = layers.Input((latent_dim_size,))\n",
    "    x = layers.Dense(4*4*512, use_bias=True)(input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "\n",
    "    x= layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU(.2)(x)\n",
    "    \n",
    "\n",
    "    x= layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU(.2)(x)\n",
    "    \n",
    "    x= layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU(.2)(x)\n",
    "\n",
    "    x= layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x= layers.LeakyReLU(.2)(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(16, (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(.2)(x)\n",
    "\n",
    "    x = Conv2D(1,3,1,'same', use_bias=True)(x)\n",
    "    output = layers.Activation('sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(input,output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_discriminator():\n",
    "\n",
    "    input = layers.Input((128,128,1))\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same')(input)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    output = layers.Activation('sigmoid')(x)\n",
    "\n",
    "\n",
    "    modelo = keras.Model(input,output)\n",
    "    \n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    label_mode=None,\n",
    "    color_mode='grayscale',\n",
    "    image_size=image_size,\n",
    "    shuffle=True,\n",
    "    seed = 1567,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "train_ds = train_ds.map(lambda x: (normalize(x)))\n",
    "\n",
    "for batch in train_ds:\n",
    "    for image in batch:\n",
    "        plt.imshow(image,cmap=plt.cm.gray)\n",
    "        plt.show()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = create_generator()\n",
    "dis = create_discriminator()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurate optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_opt = keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5)\n",
    "dis_opt = keras.optimizers.Adam(learning_rate=1e-4/2, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step():\n",
    "    gen_loss,dis_loss = 0.,0.\n",
    "    gen_loss_iter,dis_loss_iter = 0.,0.\n",
    "    for batch in train_ds:\n",
    "        \n",
    "        noise = tf.random.normal((BATCH_SIZE,latent_dim_size))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "            fake_imgs = gen(noise,training=True)\n",
    "            true_labels = dis(batch,training=True)\n",
    "            fake_labels = dis(fake_imgs,training=True)\n",
    "\n",
    "            gen_loss_iter = loss_gerador(fake_labels)\n",
    "            dis_loss_iter = loss_discriminador(true_labels,fake_labels)\n",
    "        \n",
    "        gen_gras = gen_tape.gradient(gen_loss_iter,gen.trainable_variables)\n",
    "        gen_opt.apply_gradients(zip(gen_gras,gen.trainable_variables))\n",
    "\n",
    "        dis_grads = dis_tape.gradient(dis_loss_iter,dis.trainable_variables)\n",
    "        dis_opt.apply_gradients(zip(dis_grads,dis.trainable_variables))\n",
    "\n",
    "        gen_loss += gen_loss_iter\n",
    "        dis_loss += dis_loss_iter\n",
    "        gen_loss_iter,dis_loss_iter = 0.,0.\n",
    "\n",
    "    return gen_loss/tf.cast(len(train_ds),tf.float32),dis_loss/tf.cast(len(train_ds),tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "EPOCH_SAMPLE = 10\n",
    "n = 5\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    # Histórico de Loss\n",
    "    loss_gen, loss_dis = train_step()\n",
    "    complete_hist['loss_gen'].append(loss_gen)\n",
    "    complete_hist['loss_dis'].append(loss_dis)\n",
    "    \n",
    "    # Iteração das épocas\n",
    "    if i % EPOCH_SAMPLE == 0:\n",
    "        # Print Loss\n",
    "        print(f'Ep = {i} | Loss_gen = {loss_gen:.4f}; Loss_dis = {loss_dis:.4f}')\n",
    "        # Salvar uma amostra das imagens\n",
    "        noise = tf.random.normal((n**2,latent_dim_size))\n",
    "        img_fake = gen(noise)\n",
    "        fig, ax = plt.subplots(n,n,figsize=(1,1))\n",
    "        ax = ax.ravel()\n",
    "        for ii in range(n**2):\n",
    "            ax[ii].imshow(img_fake[ii],cmap='gray')\n",
    "            ax[ii].set_axis_off()\n",
    "        plt.savefig(f'../../imgs_fake/fig{i}.png',dpi=1000)\n",
    "        plt.close()\n",
    "\n",
    "    plt.semilogy(np.array(complete_hist['loss_gen']),label=f'GEN = {loss_gen:.4f}',color='r')\n",
    "    plt.semilogy(np.array(complete_hist['loss_dis']),label=f'DIS = {loss_dis:.4f}',color='k')\n",
    "    plt.legend()\n",
    "    plt.grid(True,'minor')\n",
    "    plt.savefig('loss.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print('==================== COMPLETE ====================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
